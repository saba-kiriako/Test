<html><head><style>body {
   color: black;
}
</style></head><body><p>Let’s dive deeper into Slide 5, enhancing the explanation of the statistical methods used for drift detection, linking these methods directly to types of model drift, and providing detailed examples and Python code snippets.</p>
<h3 id="slide-3-types-of-model-drift">Slide 3: Types of Model Drift</h3>
<ul>
<li><strong>Feature Drift:</strong> Occurs when the distribution of input features changes over time.<ul>
<li><strong>Example:</strong> A loan approval model may experience feature drift if the economic conditions change, affecting customers&#39; income levels and spending behaviors.</li>
</ul>
</li>
<li><strong>Concept Drift:</strong> Happens when the relationship between the input data and the output variable changes.<ul>
<li><strong>Example:</strong> In a fraud detection system, concept drift can occur when fraudsters change their tactics, and the previous indicators of fraud no longer predict fraudulent activities correctly.</li>
</ul>
</li>
<li><strong>Label Drift:</strong> Arises when the distribution of labels changes over time, often due to external factors or changes in how labels are defined.<ul>
<li><strong>Example:</strong> In medical diagnosis models, label drift could result from changes in disease definitions or diagnostic criteria over time.</li>
</ul>
</li>
</ul>
<h3 id="slide-4-overview-of-mlflow">Slide 4: Overview of MLflow</h3>
<ul>
<li><strong>What is MLflow?</strong> An introduction to MLflow as a comprehensive open-source platform designed to manage the machine learning lifecycle, including experimentation, reproducibility, and deployment.</li>
<li><strong>Components:</strong> Detailed overview of MLflow’s core components: Tracking, Projects, Models, and Registry.</li>
</ul>
<h3 id="slide-5-statistical-methods-for-drift-detection">Slide 5: Statistical Methods for Drift Detection</h3>
<h4 id="1-chi-square-test-">1. <strong>Chi-Square Test</strong></h4>
<ul>
<li><strong>Purpose:</strong> Used primarily to detect feature drift in categorical data by comparing the observed frequencies of categories to their expected frequencies under the null hypothesis of no change in distribution.</li>
<li><strong>Explanation:</strong><ul>
<li>The Chi-Square test evaluates whether the distribution of categorical variables has changed significantly over time by measuring how observed frequencies deviate from expected frequencies.</li>
<li>It’s especially useful for identifying shifts in categorical input features which can signal the onset of feature drift.</li>
</ul>
</li>
<li><strong>Python Example:</strong><pre><code class="lang-python">from scipy.stats import chisquare
observed = [<span class="hljs-number">50</span>, <span class="hljs-number">30</span>, <span class="hljs-number">20</span>]  # Observed frequencies at current evaluation
expected = [<span class="hljs-number">40</span>, <span class="hljs-number">40</span>, <span class="hljs-number">20</span>]  # Expected frequencies based on historical data
chi_stat, p_value = chisquare(observed, expected)
print(f<span class="hljs-string">"Chi-square Stat: {chi_stat}, p-value: {p_value}"</span>)
</code></pre>
</li>
<li><strong>Application:</strong> This test can be particularly useful in applications like marketing or credit risk models, where categorical features such as customer segment or loan type are critical.</li>
</ul>
<h4 id="2-kolmogorov-smirnov-test-ks-test-">2. <strong>Kolmogorov-Smirnov Test (KS Test)</strong></h4>
<ul>
<li><strong>Purpose:</strong> A nonparametric test used to determine if two continuous data distributions differ, which is useful for detecting both feature and concept drift.</li>
<li><strong>Explanation:</strong><ul>
<li>The KS Test measures the maximum distance between the cumulative distribution functions (CDFs) of two datasets, typically the empirical distribution functions of model inputs or outputs at different times.</li>
<li>This method does not assume any specific distributional shape, making it versatile for real-world data.</li>
</ul>
</li>
<li><strong>Python Example:</strong><pre><code class="lang-python">from scipy.stats import ks_2samp
data1 = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>]  # Sample data from time T1
data2 = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.35</span>, <span class="hljs-number">0.5</span>]  # Sample data from time T2
ks_stat, p_value = ks_2samp(data1, data2)
print(f<span class="hljs-string">"KS Statistic: {ks_stat}, p-value: {p_value}"</span>)
</code></pre>
</li>
<li><strong>Application:</strong> Useful in monitoring critical continuous input or output variables like probabilities of default in a financial model or changing user engagement metrics in a digital product over time.</li>
</ul>
<h4 id="3-cramer-von-mises-criterion-">3. <strong>Cramer-von Mises Criterion</strong></h4>
<ul>
<li><strong>Purpose:</strong> Another nonparametric test that is useful for assessing the similarity of two samples’ distributions, applicable for both feature and label drift.</li>
<li><strong>Explanation:</strong><ul>
<li>Similar to the KS Test, the Cramer-von Mises criterion compares the cumulative distribution functions of two samples but considers the integrated squared difference, offering a more sensitive measure than the KS Test.</li>
</ul>
</li>
<li><strong>Python Example:</strong><pre><code class="lang-python">import numpy as np
from scipy.stats import cramervonmises_2samp
data1 = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
data2 = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2.5</span>, <span class="hljs-number">3.5</span>])
result = cramervonmises_2samp(data1, data2)
print(f<span class="hljs-string">"Statistic: {result.statistic}, p-value: {result.pvalue}"</span>)
</code></pre>
</li>
<li><strong>Application:</strong> This method can be beneficial in settings where small differences in distribution can have large impacts, such as in quality control processes or monitoring critical operational parameters in industrial applications.</li>
</ul>
<h3 id="visual-aids-for-slide-5-">Visual Aids for Slide 5:</h3>
<ul>
<li>Plots showing the distribution differences detected by each test.</li>
<li>Diagrams explaining the mechanics of each statistical test.</li>
<li>Tables summarizing test results and their implications for the model in question.</li>
</ul>
<p>Let&#39;s expand Slide 6 and Slide 7 with more comprehensive details and practical guidelines to enhance understanding.</p>
<h3 id="slide-6-automating-drift-detection-with-apache-airflow">Slide 6: Automating Drift Detection with Apache Airflow</h3>
<ul>
<li><strong>Integration of MLflow with Airflow:</strong> <ul>
<li><strong>Purpose:</strong> Airflow acts as an orchestrator for MLflow tasks, scheduling and automating the execution of model evaluations to check for drift on a regular basis.</li>
<li><strong>Benefits:</strong> Automating with Airflow ensures that drift checks are performed consistently and efficiently, minimizing manual oversight and reducing the likelihood of missed drift detections.</li>
</ul>
</li>
<li><p><strong>Example Workflow Configuration:</strong></p>
<ul>
<li><strong>Step 1:</strong> Set up the Airflow environment. This involves installing Airflow and setting up the necessary configurations, including the scheduler, executor, and web server components.</li>
<li><p><strong>Step 2:</strong> Define the Airflow DAG (Directed Acyclic Graph). A DAG is a collection of tasks you want to run, organized in a way that reflects their relationships and dependencies.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python_operator <span class="hljs-keyword">import</span> PythonOperator
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime, timedelta

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_model</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># Code to initiate MLflow, load model, and log metrics</span>
    mlflow.set_tracking_uri(<span class="hljs-string">"http://your-mlflow-server:5000"</span>)
    <span class="hljs-keyword">with</span> mlflow.start_run():
        model = mlflow.pyfunc.load_model(<span class="hljs-string">'models:/your-model/Production'</span>)
        test_data, actual_labels = fetch_recent_data()  <span class="hljs-comment"># Placeholder for actual data fetching function</span>
        predictions = model.predict(test_data)
        accuracy = compute_accuracy(predictions, actual_labels)
        mlflow.log_metric(<span class="hljs-string">"accuracy"</span>, accuracy)

default_args = {
    <span class="hljs-string">'owner'</span>: <span class="hljs-string">'airflow'</span>,
    <span class="hljs-string">'start_date'</span>: datetime(<span class="hljs-number">2023</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    <span class="hljs-string">'email_on_failure'</span>: <span class="hljs-keyword">False</span>,
    <span class="hljs-string">'retries'</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">'retry_delay'</span>: timedelta(minutes=<span class="hljs-number">5</span>),
}
dag = DAG(<span class="hljs-string">'model_drift_evaluation'</span>, default_args=default_args, schedule_interval=<span class="hljs-string">'@daily'</span>)

evaluate_task = PythonOperator(
    task_id=<span class="hljs-string">'evaluate_model_drift'</span>,
    python_callable=evaluate_model,
    dag=dag,
)
</code></pre>
</li>
<li><strong>Step 3:</strong> Schedule and monitor the DAG. Use the Airflow UI to monitor the performance and output of the scheduled tasks, adjusting parameters as necessary based on model performance and drift detection outcomes.</li>
</ul>
</li>
</ul>
<h3 id="slide-7-custom-metrics-for-drift-detection">Slide 7: Custom Metrics for Drift Detection</h3>
<ul>
<li><strong>Defining Custom Metrics:</strong><ul>
<li><strong>Purpose:</strong> Custom metrics are critical for monitoring specific aspects of a model that generic metrics may not cover, particularly for detecting subtle types of drift that might otherwise go unnoticed.</li>
<li><strong>Process:</strong> Identify the most important characteristics of the model&#39;s output or input that relate directly to business objectives or performance indicators. These characteristics will guide the development of custom metrics.</li>
</ul>
</li>
<li><p><strong>Implementation in MLflow:</strong></p>
<ul>
<li><strong>Metric Example:</strong> A custom metric for average prediction confidence, which can serve as an indicator of concept drift. This metric tracks changes in the model’s confidence in its predictions, which may signal an underlying shift in the data or concept.</li>
<li><p><strong>Python Code:</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> mlflow

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_prediction_confidence</span><span class="hljs-params">(predictions)</span>:</span>
    confidence_scores = [pred.confidence <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> predictions]  <span class="hljs-comment"># Assuming predictions have a 'confidence' attribute</span>
    average_confidence = sum(confidence_scores) / len(confidence_scores)
    mlflow.log_metric(<span class="hljs-string">"average_confidence"</span>, average_confidence)

<span class="hljs-keyword">with</span> mlflow.start_run():
    predictions = model.predict(data)  <span class="hljs-comment"># Ensure your model returns confidence scores</span>
    log_prediction_confidence(predictions)
</code></pre>
</li>
</ul>
</li>
<li><strong>Practical Considerations:</strong> <ul>
<li><strong>Setting Thresholds:</strong> Determine thresholds for these metrics that, when crossed, will trigger investigations or actions. For example, a significant drop in prediction confidence might necessitate a model retraining or further data analysis.</li>
<li><strong>Integration with Monitoring:</strong> Seamlessly integrate these custom metrics into your overall monitoring framework to ensure continuous observation and timely response to potential drift.</li>
</ul>
</li>
</ul>
</body></html>


<html><head><style>body {
   color: black;
}
</style></head><body><p>MLflow is a powerful tool for managing the machine learning lifecycle, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models. When using XGBoost with MLflow, there are several metrics and parameters you can monitor and log to evaluate the performance of your models. Here&#39;s a breakdown of typical metrics and parameters you might track:</p>
<h3 id="metrics-to-monitor">Metrics to Monitor</h3>
<ol>
<li><strong>Accuracy</strong>: This is the proportion of correctly predicted observations to the total observations and is particularly useful for classification problems.</li>
<li><strong>Precision, Recall, and F1-Score</strong>: These metrics provide a more detailed insight into classification model performance, especially when dealing with imbalanced datasets.</li>
<li><strong>AUC-ROC</strong>: The area under the receiver operating characteristic curve; a higher AUC-ROC score represents a model with better performance at distinguishing between the positive and negative classes.</li>
<li><strong>Log Loss</strong>: Particularly useful for binary classification problems, log loss provides a measure of accuracy where the prediction input is a probability value between 0 and 1.</li>
<li><strong>Mean Absolute Error (MAE)</strong> and <strong>Mean Squared Error (MSE)</strong>: Common metrics for regression tasks, providing information on the average magnitude of errors in a set of predictions.</li>
<li><strong>Root Mean Squared Error (RMSE)</strong>: Measures the standard deviation of the residuals (prediction errors) in regression.</li>
</ol>
<h3 id="parameters-to-log">Parameters to Log</h3>
<ol>
<li><strong>Learning Rate</strong>: This is a hyperparameter that controls how much the model is adjusted based on the error each time the model weights are updated.</li>
<li><strong>Number of Trees (n_estimators)</strong>: In gradient boosting models like XGBoost, this parameter refers to the number of trees you build (an ensemble of trees).</li>
<li><strong>Max Depth</strong>: The maximum depth of a tree. Increasing this value will make the model more complex and more likely to fit better to the training data, possibly leading to overfitting.</li>
<li><strong>Subsample</strong>: The fraction of samples to be used for fitting the individual base learners. Setting it below 1.0 can lead to a reduction of variance and an increase in bias.</li>
<li><strong>ColSample by Tree/Level/Node</strong>: These parameters control the subsampling of columns before constructing each tree (bytree), at each split (bylevel), or at each node (bynode). They are used for adding more randomness to make training robust to noise.</li>
</ol>
<h3 id="how-to-use-mlflow-with-xgboost">How to Use MLflow with XGBoost</h3>
<p>To use MLflow with XGBoost, you typically:</p>
<ol>
<li><strong>Initialize MLflow</strong>: Start by setting up MLflow and creating an experiment.</li>
<li><strong>Log Parameters</strong>: Use <code>mlflow.log_param()</code> to log various hyperparameters of the XGBoost model.</li>
<li><strong>Train Model</strong>: Train your XGBoost model as usual.</li>
<li><strong>Log Metrics</strong>: After training, use <code>mlflow.log_metric()</code> to log important metrics like RMSE, MAE, or accuracy.</li>
<li><strong>Log Model</strong>: Save and log the XGBoost model itself using <code>mlflow.xgboost.log_model()</code>.</li>
</ol>




<p>For managing and visualizing different types of logs and metrics from your Kedro data science project alongside MLflow, you can consider integrating the different tools you have (MLflow, Elasticsearch, Kibana, and Grafana) into a cohesive monitoring dashboard. Each tool offers distinct capabilities that can be utilized for comprehensive monitoring and visualization:</p>
<ol>
<li><p><strong>MLflow</strong>: Primarily used for tracking experiments, including logging parameters, metrics, and outputs of models. MLflow is focused on the machine learning lifecycle and is effective for comparing different runs and experiments.</p>
</li>
<li><p><strong>Elasticsearch and Kibana</strong>:</p>
<ul>
<li><strong>Elasticsearch</strong>: A search and analytics engine ideal for indexing, searching, and analyzing large volumes of data quickly. In your case, it can be used to store and query technical logs from your Python code.</li>
<li><strong>Kibana</strong>: Works on top of Elasticsearch and is used for visualizing data in Elasticsearch indexes. It can create dashboards showing logs, metrics, and other data-driven insights.</li>
</ul>
</li>
<li><p><strong>Grafana</strong>:</p>
<ul>
<li><strong>Grafana</strong>: A powerful tool for building complex monitoring dashboards. It can integrate with a wide range of data sources including Elasticsearch and, through plugins, MLflow. Grafana is particularly strong in creating visually appealing dashboards and has extensive support for alerting and multi-data source integration.</li>
</ul>
</li>
</ol>
<h3 id="integration-strategy">Integration Strategy</h3>
<p>To visualize everything in a single tool, Grafana is a strong candidate due to its ability to pull data from multiple sources and provide a unified dashboard experience. Here’s how you can approach it:</p>
<ul>
<li><p><strong>Log Technical Logs to Elasticsearch</strong>: Configure your Python applications to send logs to Elasticsearch. This might involve setting up log handlers in Python that write directly to Elasticsearch or using intermediary log shippers like Logstash or Fluentd.</p>
</li>
<li><p><strong>Use MLflow for Experiment Tracking</strong>: Continue using MLflow for tracking experiments, models, parameters, and metrics. Ensure that MLflow’s tracking server is properly set up and accessible.</p>
</li>
<li><p><strong>Visualize Data in Grafana</strong>:</p>
<ul>
<li><strong>Integrate MLflow with Grafana</strong>: Although Grafana doesn’t have native support for MLflow, you can use a community-developed plugin or API to fetch data from MLflow. Alternatively, store MLflow metrics in a database that Grafana supports.</li>
<li><strong>Elasticsearch as a DataSource in Grafana</strong>: Add Elasticsearch as a data source in Grafana. This allows you to create panels and dashboards in Grafana that query and display data from Elasticsearch, including your technical logs.</li>
<li><strong>Unified Dashboards</strong>: Create dashboards in Grafana that incorporate both the experiment metrics from MLflow (via a compatible database or plugin) and the logs from Elasticsearch. This way, you can visualize machine learning metrics alongside application logs and system metrics.</li>
</ul>
</li>
<li><p><strong>Leverage Kibana for Deep Dives</strong>: While Grafana provides an excellent overview and high-level dashboard capabilities, Kibana can be used for more in-depth exploration of logs stored in Elasticsearch. You might use Kibana for creating specific log analysis visualizations or for utilizing its machine learning features for anomaly detection in log data.</p>
</li>
</ul>
<p>By using Grafana as your central visualization tool, you can create a comprehensive overview of both operational and experimental aspects of your data science projects. This setup helps in correlating logs with model performance and understanding the impact of different parameters and system behaviors on your machine learning models.</p>


   <p>Storing MLflow logs in Elasticsearch is technically feasible, but it&#39;s important to consivder the purpose and structure of both tools to determine if it&#39;s the best approach for your use case.</p>
<h3 id="mlflow-s-components-and-storage-">MLflow&#39;s Components and Storage:</h3>
<p>MLflow comprises several components including tracking experiments, projects, models, and a model registry. The tracking component, which you would likely consider for integrating with Elasticsearch, involves parameters, metrics, and artifacts (such as model files or plots).</p>
<ul>
<li><strong>Parameters and Metrics</strong>: These are typically key-value pairs and can easily be serialized into a format that can be indexed by Elasticsearch.</li>
<li><strong>Artifacts</strong>: These can be files of any type and size. Storing them directly in Elasticsearch isn&#39;t practical due to the nature of the storage and querying capabilities in Elasticsearch, which are optimized for text-based or numerical data rather than binary files.</li>
</ul>
<h3 id="pros-and-cons-of-using-elasticsearch-for-mlflow-logs-">Pros and Cons of Using Elasticsearch for MLflow Logs:</h3>
<p><strong>Pros:</strong></p>
<ul>
<li><strong>Centralized Logging</strong>: Integrating MLflow logs into Elasticsearch could centralize your data storage, making it easier to search and analyze data across different sources (e.g., application logs, system metrics, MLflow metrics).</li>
<li><strong>Advanced Search Capabilities</strong>: Elasticsearch provides powerful full-text search capabilities that can be useful if you need complex search queries on your experiment metadata.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li><strong>Complexity</strong>: Adding Elasticsearch as a store for MLflow logs introduces additional complexity in terms of data synchronization and maintenance.</li>
<li><strong>Data Duplication</strong>: Storing data both in MLflow&#39;s backend and in Elasticsearch could lead to redundancy and increased storage requirements.</li>
<li><strong>Not Optimized for Binary Data</strong>: As mentioned, storing artifacts in Elasticsearch is not practical, so you would need to handle artifacts differently or only store references to them in Elasticsearch.</li>
</ul>
<h3 id="alternative-approaches-">Alternative Approaches:</h3>
<ol>
<li><p><strong>Use Elasticsearch for What It’s Best At</strong>: Keep using MLflow with its default storage backend (like a file store or a database) for experiment tracking, and use Elasticsearch exclusively for logging application or operational logs. You can link data between Elasticsearch and MLflow by using common identifiers (e.g., experiment IDs, run IDs).</p>
</li>
<li><p><strong>Visualizing Across Tools</strong>: Use a tool like Grafana to create dashboards that pull data from both MLflow and Elasticsearch, providing a unified view without needing to store all data in a single place.</p>
</li>
<li><p><strong>Integration via API or Middleware</strong>: Instead of storing logs directly, consider developing a middleware or using an API that fetches necessary data from MLflow and indexes it into Elasticsearch when needed. This can be done periodically or triggered by specific events, thus keeping Elasticsearch as an up-to-date search layer rather than a primary data store.</p>
</li>
</ol>
<h3 id="conclusion">Conclusion</h3>
<p>While it&#39;s possible to store MLflow logs in Elasticsearch, it might not always be the best or the simplest solution. Evaluating your specific needs for data access, search capabilities, and operational management will help determine the best architecture for your project.</p>


This process can be automated and improved based on specific requirements, such as handling large datasets, dealing with network issues, or optimizing performance. Always ensure that you have proper backups before starting the transfer process.
<p>This setup allows you to track how changes in hyperparameters affect model performance and compare different runs within an organized framework.</p>


Certainly! Below are two Python scripts: one for extracting all data from an MLflow server and another for uploading the extracted data to another MLflow server.

### Script 1: Extract Everything from MLflow

This script will extract experiments, runs, and artifacts from the MLflow server and save them to local files.

```python
import mlflow
from mlflow.tracking import MlflowClient
import json
import os
import shutil

# Set the MLflow tracking URI to point to your MLflow server
mlflow.set_tracking_uri("http://first_mlflow_server:5000")
client = MlflowClient()

# Directory to save extracted data
output_dir = "mlflow_extraction"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Extract experiments
experiments = client.list_experiments()
experiments_data = []

for experiment in experiments:
    experiment_data = {
        "experiment_id": experiment.experiment_id,
        "name": experiment.name,
        "artifact_location": experiment.artifact_location,
        "lifecycle_stage": experiment.lifecycle_stage
    }
    experiments_data.append(experiment_data)

    # Extract runs for each experiment
    runs = client.search_runs(experiment_ids=[experiment.experiment_id])
    runs_data = []

    for run in runs:
        run_data = {
            "run_id": run.info.run_id,
            "status": run.info.status,
            "artifact_uri": run.info.artifact_uri,
            "start_time": run.info.start_time,
            "end_time": run.info.end_time,
            "metrics": run.data.metrics,
            "params": run.data.params,
            "tags": run.data.tags
        }
        runs_data.append(run_data)

        # Extract artifacts for each run
        artifacts_dir = os.path.join(output_dir, experiment.name, run.info.run_id, "artifacts")
        if not os.path.exists(artifacts_dir):
            os.makedirs(artifacts_dir)
        artifacts = client.list_artifacts(run.info.run_id)
        for artifact in artifacts:
            local_path = client.download_artifacts(run.info.run_id, artifact.path, artifacts_dir)

    # Save runs data to file
    runs_file_path = os.path.join(output_dir, experiment.name, "runs.json")
    with open(runs_file_path, "w") as f:
        json.dump(runs_data, f, indent=4)

# Save experiments data to file
experiments_file_path = os.path.join(output_dir, "experiments.json")
with open(experiments_file_path, "w") as f:
    json.dump(experiments_data, f, indent=4)

print(f"Extraction completed. Data saved to {output_dir}")
```

### Script 2: Upload Everything to MLflow

This script will upload the extracted data to another MLflow server.

```python
import mlflow
from mlflow.tracking import MlflowClient
import json
import os

# Set the MLflow tracking URI to point to your MLflow server
mlflow.set_tracking_uri("http://second_mlflow_server:5000")
client = MlflowClient()

# Directory containing the extracted data
input_dir = "mlflow_extraction"

# Load experiments data from file
experiments_file_path = os.path.join(input_dir, "experiments.json")
with open(experiments_file_path, "r") as f:
    experiments_data = json.load(f)

for experiment_data in experiments_data:
    # Create experiment on the new server
    experiment_id = client.create_experiment(
        name=experiment_data["name"],
        artifact_location=experiment_data["artifact_location"]
    )

    # Load runs data from file
    runs_file_path = os.path.join(input_dir, experiment_data["name"], "runs.json")
    with open(runs_file_path, "r") as f:
        runs_data = json.load(f)

    for run_data in runs_data:
        with mlflow.start_run(experiment_id=experiment_id, run_name=run_data["run_id"]):
            # Log metrics, params, and tags
            for metric, value in run_data["metrics"].items():
                mlflow.log_metric(metric, value)
            for param, value in run_data["params"].items():
                mlflow.log_param(param, value)
            for tag, value in run_data["tags"].items():
                mlflow.set_tag(tag, value)

            # Log artifacts
            artifacts_dir = os.path.join(input_dir, experiment_data["name"], run_data["run_id"], "artifacts")
            if os.path.exists(artifacts_dir):
                for root, _, files in os.walk(artifacts_dir):
                    for file in files:
                        artifact_path = os.path.relpath(os.path.join(root, file), artifacts_dir)
                        mlflow.log_artifact(os.path.join(root, file), artifact_path)

print("Upload completed.")
```

### Instructions to Use the Scripts

1. **Run the Extraction Script**:
    - Set the tracking URI in the extraction script to point to your first MLflow server.
    - Execute the script to extract data.

    ```bash
    python extract_mlflow_data.py
    ```

2. **Run the Upload Script**:
    - Set the tracking URI in the upload script to point to your second MLflow server.
    - Execute the script to upload data.

    ```bash
    python upload_mlflow_data.py
    ```

These scripts will allow you to extract all experiments, runs, and artifacts from one MLflow server and upload them to another MLflow server. Make sure the directories and paths in the scripts match your local setup.
</body></html>


import shutil
import os

def zip_folder(folder_path, output_path):
    """
    Zips the contents of a folder.

    Parameters:
    folder_path (str): Path to the folder to be zipped.
    output_path (str): Output path for the zip file (without .zip extension).
    """
    # Ensure the folder exists
    if not os.path.exists(folder_path):
        print(f"Error: The folder {folder_path} does not exist.")
        return

    # Create a zip file
    shutil.make_archive(output_path, 'zip', folder_path)
    print(f"Folder {folder_path} has been zipped to {output_path}.zip")

# Example usage
folder_to_zip = 'path/to/your/folder'
zip_output_path = 'path/to/output/folder_name'
zip_folder(folder_to_zip, zip_output_path)







####new


import shutil
import os

def unzip_file(zip_path, extract_to):
    """
    Unzips the contents of a zip file.

    Parameters:
    zip_path (str): Path to the zip file to be extracted.
    extract_to (str): Path to the directory where files will be extracted.
    """
    # Ensure the zip file exists
    if not os.path.exists(zip_path):
        print(f"Error: The zip file {zip_path} does not exist.")
        return

    # Ensure the extract directory exists
    if not os.path.exists(extract_to):
        os.makedirs(extract_to)

    # Extract the zip file
    shutil.unpack_archive(zip_path, extract_to)
    print(f"Extracted {zip_path} to {extract_to}")

# Example usage
zip_file_path = 'path/to/your/zipfile.zip'
extract_to_path = 'path/to/extract/directory'
unzip_file(zip_file_path, extract_to_path)






import mlflow
from mlflow.tracking import MlflowClient
import json
import os

# Set the MLflow tracking URI to point to your MLflow server
mlflow.set_tracking_uri("http://first_mlflow_server:5000")
client = MlflowClient()

# Directory to save extracted data
output_dir = "mlflow_extraction"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Extract registered models
models = client.list_registered_models()
models_data = []

for model in models:
    model_data = {
        "name": model.name,
        "creation_timestamp": model.creation_timestamp,
        "last_updated_timestamp": model.last_updated_timestamp,
        "description": model.description,
        "latest_versions": []
    }

    for version in model.latest_versions:
        version_data = {
            "version": version.version,
            "run_id": version.run_id,
            "status": version.status,
            "stage": version.current_stage,
            "creation_timestamp": version.creation_timestamp,
            "last_updated_timestamp": version.last_updated_timestamp,
            "source": version.source,
            "run_link": version.run_link
        }
        model_data["latest_versions"].append(version_data)

        # Download model artifacts
        model_artifacts_dir = os.path.join(output_dir, "models", model.name, version.version)
        if not os.path.exists(model_artifacts_dir):
            os.makedirs(model_artifacts_dir)
        client.download_artifacts(version.run_id, "", model_artifacts_dir)

    models_data.append(model_data)

# Save models data to file
models_file_path = os.path.join(output_dir, "models.json")
with open(models_file_path, "w") as f:
    json.dump(models_data, f, indent=4)

print(f"Extraction of registered models completed. Data saved to {output_dir}")












import mlflow
from mlflow.tracking import MlflowClient
import json
import os

# Set the MLflow tracking URI to point to your MLflow server
mlflow.set_tracking_uri("http://second_mlflow_server:5000")
client = MlflowClient()

# Directory containing the extracted data
input_dir = "mlflow_extraction"

# Load models data from file
models_file_path = os.path.join(input_dir, "models.json")
with open(models_file_path, "r") as f:
    models_data = json.load(f)

for model_data in models_data:
    # Create the registered model on the new server
    client.create_registered_model(model_data["name"])
    
    # Add description to the model if exists
    if model_data["description"]:
        client.update_registered_model(
            name=model_data["name"],
            description=model_data["description"]
        )
    
    for version_data in model_data["latest_versions"]:
        # Create a new version of the model
        new_version = client.create_model_version(
            name=model_data["name"],
            source=version_data["source"],
            run_id=version_data["run_id"]
        )
        
        # Update model version stage
        client.transition_model_version_stage(
            name=model_data["name"],
            version=new_version.version,
            stage=version_data["stage"]
        )
        
        # Add description to the model version if exists
        if version_data["description"]:
            client.update_model_version(
                name=model_data["name"],
                version=new_version.version,
                description=version_data["description"]
            )
        
        # Download and log artifacts for the new version
        model_artifacts_dir = os.path.join(input_dir, "models", model_data["name"], version_data["version"])
        if os.path.exists(model_artifacts_dir):
            for root, _, files in os.walk(model_artifacts_dir):
                for file in files:
                    artifact_path = os.path.relpath(os.path.join(root, file), model_artifacts_dir)
                    mlflow.log_artifact(os.path.join(root, file), artifact_path)

print("Upload of registered models and versions completed.")
